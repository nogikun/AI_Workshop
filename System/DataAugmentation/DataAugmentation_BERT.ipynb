{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TizQdzE4_xe8"
      },
      "source": [
        "# Excelで入力した教師データの拡張\n",
        "\n",
        "編集日：2023/12/03 <br>\n",
        "編集者：[nogikun](https://github.com/nogikun) <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPsVxebr_xe-"
      },
      "source": [
        "#### 期待するファイル\n",
        "以下のようなテーブルを含むExcelファイルを用意してください。<br>\n",
        "シート数は１つで、シート名は任意です。\n",
        "\n",
        "※ `system` はA1セル、`user` はB1セル、`assistant` はC1セルに入力してください。\n",
        "\n",
        "|system|user|assistant|\n",
        "|:-:|:-:|:-:|\n",
        "|あなたはスタックチャンです。おはなしをしよう。|こんにちは|こんにちは僕の名前はスタックチャンです|\n",
        "|あなたはスタックチャンです。おはなしをしよう。|{任意の質問}|{任意の質問に対応する解答}|\n",
        "|︙|︙|︙|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. ライブラリのインポート・関数定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# BERT系\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "# MeCab\n",
        "!pip install mecab-python3\n",
        "# unidicを使う\n",
        "!pip install unidic\n",
        "!python -m unidic download\n",
        "# ipadicを使う\n",
        "!pip install ipadic\n",
        "# データフレーム\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#\n",
        "# 当セルはランタイムにつき1回の実行でOKです\n",
        "#\n",
        "\n",
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import torch\n",
        "import sentencepiece\n",
        "import MeCab\n",
        "import unidic\n",
        "import ipadic\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "# BERTモデルのインスタンス化\n",
        "pipe = pipeline(\"fill-mask\", model=\"nlp-waseda/roberta-base-japanese\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"nlp-waseda/roberta-base-japanese\")\n",
        "\n",
        "\n",
        "# 分かち書きを行う関数\n",
        "def wakachi(text = '早稲田大学で自然言語処理を専攻する。'): # 引数に入っている初期値は想定する入力\n",
        "  # 分かち書き\n",
        "  DATA = np.array([['','']])\n",
        "  mecab = MeCab.Tagger(ipadic.MECAB_ARGS)\n",
        "  tokens = mecab.parse(text)\n",
        "\n",
        "  for token in tokens.split('\\n'):\n",
        "    data = token.split('\\t')\n",
        "    try:\n",
        "      # print([[data[0],data[1].split(',')[0]]])\n",
        "      DATA = np.append(DATA, [[data[0],data[1].split(',')[0]]],0)\n",
        "    except:\n",
        "      pass\n",
        "  DATA=DATA[1:]\n",
        "\n",
        "  texts = []\n",
        "  for i in range(len(DATA.T[0])):\n",
        "    if DATA.T[1][i] in ['名詞']:\n",
        "      texts.append(' '.join(list(DATA.T[0][:i])+['[MASK]']+list(DATA.T[0][i+1:])))\n",
        "  return texts\n",
        "\n",
        "# BERTによる予測を行う関数\n",
        "def word_prediction(text = \"早稲田 大学 で 自然 言語 処理 を [MASK] する 。\",add_num = 10): # 引数に入っている初期値は想定する入力\n",
        "  input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
        "  mask_idx = input_ids.tolist()[0].index(tokenizer.mask_token_id)\n",
        "  with torch.no_grad():\n",
        "      prediction = model(input_ids).logits\n",
        "\n",
        "  # Get the probabilities using softmax\n",
        "  probs = torch.nn.functional.softmax(prediction[0, mask_idx], dim=0)\n",
        "\n",
        "  # Get the top 10 predicted token ids and their probabilities\n",
        "  top_10_token_ids = torch.topk(probs, add_num).indices\n",
        "  top_10_probs = torch.topk(probs, add_num).values\n",
        "\n",
        "  predicted_words = [tokenizer.decode([token_id]) for token_id in top_10_token_ids]\n",
        "\n",
        "  # Zip the words with their probabilities and print them\n",
        "  for word, prob in zip(predicted_words, top_10_probs):\n",
        "      pass\n",
        "      #print(f\"{word}: {prob.item():.5f}\")\n",
        "\n",
        "  return predicted_words  # zip(predicted_words, top_10_probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYBjEg7I_xe_"
      },
      "source": [
        "#### 2.実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = '..' # 同一でない場合は適宜変更する。\n",
        "DATA = pd.read_excel(f'{file_path}/FineTuningData.xlsx') # ファイル名は適宜変更する。\n",
        "\n",
        "for i in range(len(DATA['user'])):\n",
        "  #input(DATA['user'][i])\n",
        "\n",
        "  # 言い換え文を作成\n",
        "  new_keywords = []\n",
        "  for text in wakachi(text = DATA['user'][i]): # userの入力文を与える\n",
        "    #input(text)\n",
        "    for word in word_prediction(text,30):\n",
        "      if word not in  ['。','、','?','!','.','...','......','−','–','――','「','」','～','(',')','『','”',':']:\n",
        "        new_keywords.append( text.replace('[MASK]', word).replace(' ', '') )\n",
        "\n",
        "\n",
        "  #new_keywords = ['a','b','c']\n",
        "\n",
        "  # 拡張したデータを追加\n",
        "  for new_keyword in new_keywords:\n",
        "    new_data = pd.DataFrame([{\n",
        "        'system':DATA['system'][i],\n",
        "        'user':new_keyword, # 書き換えた入力を加える\n",
        "        'assistant':DATA['assistant'][i]\n",
        "        }])\n",
        "    DATA = pd.concat([DATA, new_data], ignore_index=True)\n",
        "\n",
        "\n",
        "# チューニングデータを作成\n",
        "TunigData = []\n",
        "for i in DATA.T:\n",
        "  append_json = {\n",
        "      'messages': [\n",
        "          {'role': 'system','content': DATA.T[i]['system']},\n",
        "           {'role': 'user', 'content': DATA.T[i]['user']},\n",
        "            {'role': 'assistant','content': DATA.T[i]['assistant']}\n",
        "      ]}\n",
        "  TunigData.append(append_json)\n",
        "\n",
        "with open('TuningData.jsonl', 'w') as file:\n",
        "  for entry in TunigData:\n",
        "      file.write(json.dumps(entry) + '\\n')\n",
        "\n",
        "DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
